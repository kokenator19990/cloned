# ğŸ§  Como Usar el Razonamiento Cognitivo con Ollama

## âœ… Ya Tienes Todo Funcionando

### **Infraestructura lista**:
- âœ… **Docker**: PostgreSQL + Redis + MinIO corriendo
- âœ… **Backend API**: NestJS en puerto 3001 (health check OK)
- âœ… **Ollama**: Llama3 instalado y funcionando en puerto 11434
- âœ… **Frontend**: Next.js en puerto 3000

---

## ğŸ¯ CÃ³mo Funciona Ahora

### **Modo 1: Local Simple** (actual)
**Ruta**: `/clones/{id}/chat`

**QuÃ© hace**:
- âœ… Matching de keywords en las respuestas guardadas
- âœ… Inferencia por categorÃ­as
- âœ… Reproduce grabaciones originales
- âš ï¸ **LimitaciÃ³n**: No usa LLM, solo bÃºsqueda semÃ¡ntica simple

**CuÃ¡ndo usarlo**: 
- Para probar rÃ¡pido sin backend
- Si Ollama no estÃ¡ disponible
- Modo offline

---

### **Modo 2: Razonamiento LLM con Ollama** (NUEVO ğŸš€)
**Ruta**: `/clones/{id}/chat-hybrid`

**QuÃ© hace**:
- ğŸ§  **Usa Ollama** (Llama3) para razonar como la persona
- ğŸ“ Construye un **system prompt cognitivo** con todas las respuestas
- ğŸ” El LLM "piensa" cÃ³mo responderÃ­a esa persona especÃ­fica
- ğŸ’¾ Guarda cada interacciÃ³n en la base de datos
- ğŸ™ï¸ (Futuro) Sintetiza con voz clonada

**CuÃ¡ndo usarlo**:
- Cuando quieres respuestas **coherentes y con razonamiento**
- Para que el clon "piense" como la persona
- Conversaciones largas y contextuales

---

## ğŸš€ Como Probarlo AHORA

### **Paso 1: Verificar que todo estÃ© corriendo**

#### Terminal 1 - Docker (ya estÃ¡):
```bash
docker ps
# DeberÃ­as ver: deadbot-postgres, deadbot-redis, deadbot-minio
```

#### Terminal 2 - API (ya estÃ¡):
```bash
# Ya estÃ¡ corriendo en puerto 3001
# http://localhost:3001/health deberÃ­a responder OK
```

#### Terminal 3 - Frontend (ya estÃ¡):
```bash
# Ya estÃ¡ corriendo en puerto 3000
```

#### Terminal 4 - Ollama:
```bash
ollama list
# DeberÃ­as ver: llama3:latest
```

---

### **Paso 2: Crear un perfil y responder preguntas**

1. **Abre**: http://localhost:3000
2. **Crea un clon**: BotÃ³n "Nuevo Clon"
3. **Responde al menos 25 preguntas** (mejor si son 50+)
4. **Usa grabaciones de voz** para mÃ¡s autenticidad
5. **Completa el onboarding**

---

### **Paso 3: Probar ambos modos**

#### **Modo Simple (sin LLM)**:
1. Ve al chat normal: `/clones/{id}/chat`
2. Pregunta algo: "Â¿CÃ³mo eres cuando te enojas?"
3. Responde con **matching de keywords**

#### **Modo LLM (con Ollama)** ğŸ§ :
1. **Clic en el botÃ³n morado âš¡** en el header del chat
2. O ve directamente a: `/clones/{id}/chat-hybrid`
3. Pregunta lo mismo: "Â¿CÃ³mo eres cuando te enojas?"
4. El LLM **razona** usando TODO el perfil cognitivo

**Diferencia**:
- **Simple**: Busca respuesta mÃ¡s cercana
- **LLM**: Analiza el perfil completo y genera respuesta **como pensarÃ­a esa persona**

---

## ğŸ”¥ Arquitectura del Razonamiento

### **Flujo LLM**:

```
Usuario: "Â¿QuÃ© opinas del dinero?"
        â†“
Frontend â†’ Backend API
        â†“
ChatService.sendMessage()
        â†“
1. Busca memories relevantes (pgvector)
2. Construye system prompt:
   ```
   Eres {Nombre}. Responde EXACTAMENTE como esta persona.
   
   [PERSONALIDAD]
   - Soy tranquilo pero puedo enojarme rÃ¡pido
   - Me gusta la honestidad
   
   [VALORES]
   - El dinero es importante pero no lo mÃ¡s importante
   - Prefiero ser feliz que rico
   
   [...]
   ```
3. LLM (Ollama) genera respuesta
        â†“
Backend responde: "Mira, el dinero para mÃ­ es una herramienta.
                   No voy a mentirte, me gusta tener para vivir
                   tranquilo, pero jamÃ¡s sacrificarÃ­a mi tiempo
                   con la familia por acumularlo..."
        â†“
Frontend reproduce + TTS
```

---

## ğŸ“Š ComparaciÃ³n

| Feature | Modo Simple | Modo LLM |
|---------|-------------|----------|
| Velocidad | âš¡ InstantÃ¡neo | ğŸ• 2-5 seg |
| Coherencia | â­â­ Matching bÃ¡sico | â­â­â­â­â­ Razonamiento real |
| Contexto | âŒ Solo respuesta actual | âœ… Todo el perfil |
| Aprendizaje | âŒ EstÃ¡tico | âœ… Mejora con interacciones |
| Voz real | âœ… Grabaciones originales | âœ… + (futuro) voz clonada |
| Offline | âœ… | âŒ (requiere Ollama) |

---

## ğŸ“ Por QuÃ© el LLM Es Mejor

### **Ejemplo Real**:

**Perfil Jorge**: 
- RespondiÃ³: "Me enojo cuando me mienten"
- RespondiÃ³: "Soy tranquilo pero explosivo"
- RespondiÃ³: "Valoro mucho la honestidad"

**Pregunta**: "Â¿QuÃ© harÃ­as si tu amigo te mintiÃ³?"

#### **Modo Simple**:
Busca keyword "mentir" â†’ encuentra "Me enojo cuando me mienten" â†’ responde textual

**Respuesta**: "Me enojo cuando me mienten"

#### **Modo LLM**:
Lee TODO el perfil â†’ Entiende que:
- Jorge valora honestidad
- Se enoja con mentiras
- Es tranquilo pero puede explotar

**Respuesta**: "Mira, si un amigo me miente, obviamente me va a molestar. Soy muy tranquilo normalmente, pero la honestidad es re importante para mÃ­. Dependiendo de quÃ© tan grave sea, probablemente le dirÃ­a las cosas claras. No soy de los que explotan al toque, pero sÃ­ le harÃ­a saber que eso no estÃ¡ bien y que me decepcionÃ³."

**Ver la diferencia**? El LLM:
- âœ… Infiere comportamientos combinando mÃºltiples respuestas
- âœ… Responde con su estilo ("re importante", "al toque")
- âœ… Mantiene coherencia con su personalidad

---

## ğŸš€ PrÃ³ximos Pasos

### **Ya implementado**:
- [x] Auto-login sin fricciÃ³n
- [x] Hook de chat con backend
- [x] System prompt cognitivo
- [x] Toggle entre local/LLM
- [x] BotÃ³n âš¡ en chat

### **Para pulir mÃ¡s (opcionales)**:
- [ ] Migrar creaciÃ³n de perfiles al backend (para persistencia)
- [ ] Subir audios a MinIO
- [ ] Voice cloning con ElevenLabs
- [ ] Streaming de respuestas (para que se vea escribiendo)
- [ ] Memoria a largo plazo (guarda conversaciones)

---

## ğŸ› Troubleshooting

### **"El modo LLM no responde"**
```bash
# 1. Verifica que Ollama estÃ© corriendo
ollama list

# 2. Verifica que el backend pueda conectarse
curl http://localhost:11434/api/tags

# 3. Chequea logs del backend
# En la terminal del API busca errores
```

### **"Ollama estÃ¡ lento"**
- Llama3 usa ~4.7GB de RAM
- Primera respuesta es mÃ¡s lenta (carga el modelo)
- Respuestas siguientes son mÃ¡s rÃ¡pidas (modelo en memoria)

### **"Quiero usar GPT-4 en vez de Llama3"**
Edita `.env` del backend:
```bash
LLM_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=sk-tu-key-de-openai
LLM_MODEL=gpt-4o-mini
```

---

## ğŸ“ Resumen

### **Modo Simple**: 
BÃºsqueda rÃ¡pida, offline, bueno para MVP

### **Modo LLM**: 
Razonamiento real, el clon "piensa" como la persona, mejor experiencia

### **RecomendaciÃ³n**:
Usa **LLM para demos y uso real**. El modo simple queda como fallback.

---

## ğŸ¯ Siguiente Nivel

Cuando quieras que el clon hablÃ© con **su voz real** (no TTS robÃ³tico):

1. **ElevenLabs** (fÃ¡cil, rÃ¡pido, ~$5/mes)
2. **Coqui TTS** (gratis, open source, requiere GPU)

Ver: `VOICE_CLONING_ROADMAP.md`

---

**ProbÃ¡ ahora**:
1. http://localhost:3000/clones
2. Entra a un clon
3. Clic en âš¡ (botÃ³n morado)
4. Preguntale algo complejo
5. ComparÃ¡ con el modo normal

**Vas a ver la diferencia**. ğŸš€
